# x) Read and summarize

## The widely acclaimed white paper "Intelligence-Driven Computer Network Defense Informed by Analysis of Adversary Campaigns and Intrusion Kill Chains" by Eric M. Hutchins∗, Michael J. Cloppert†, Rohan M. Amin, Ph.D.‡ in Lockheed Martin Corporation describes the idea of applying the kill chain model into intrusion detection. 

The model defines the following stages of of cyber attacks: reconnaissance, weaponization, delivery, exploitation, installation, Command and Control, actions on objectives.

The white paper defines Advanced Persistent Threats (APTs) as sophisticated, systematic cyber attack campaigns that are typically conducted by nation-states or state-sponsored groups. The framework can be used to analyze APTs by breaking down their complex attacks into manageable stages, which can then be targeted with specific defensive measures.

I work in IT industry as an engineer and from my perspective the topics included in this white paper are more relevant than ever during my career. The APT is the most dangerous adversary since every system is possible to breach and you can even think the situation in a way that as the time moves forward, the APT will surely find a way to get in to the organization at some point. And as these adversaries are state backed or sponsored the resources on time can be limitless and these campaigns can take years and operate in real life relying on social engineering and human error and also on technical level trying to leverage misconfigurations in the target organisations systems. 

The Cyber Kill Chain works as a model when you're trying to define the attack surface of the target organisation and when you are planning the security controls. Understanding the different stages helps in adopting the defence in depth and mitigating risks involved every stage is necessary when defending against the most impactful adversaries. 

## Shostack 2014: Chapter 1 - Dive In and Threat Model presents the idea that Threat Modelling is a skill that everyone can learn and apply in their lives. It simplifies the idea of 'Threat Modelling' into something that's "about using models to find security problems".

It also presents tools such as diagrams and card games as tools to asses risks and to communicate what you are building. To Threat Model your own you can follow these easy steps by the book:

1. Draw a diagram.
2. Use the EoP game to find threats.
3. Address each threat in some way.
4. Check your work with the checklists at the end of this chapter.
5. Celebrate and share your work.

I think threat modelling is a great way for technical personnel and developers to implement "shift-left" type of culture where risks related to technical aspects happen in the early planning stage of SDLC (Software Development Life Cycle) or DevOps cycle. It's still very common in many organisations IT operations that security is something that's considered as an afterthought and assessed by audits in existin environments. Tilting the security matters to the beginning of the life cycle doesn't exclude the need for these kind of assessments as security is something that mature organisations embed into every stage of the lifecycle. However, doing a profound threat modelling and recognizing risks and needs for technical security controls makes it possible to address these flaws before they are deployed and therefore reducing the security risks significantly. 

## Karvinen 2023: Install Debian on Virtualbox is a very easy to understand and user friendly guide on how you can install Debian on VirtualBox, so you can run Linux on your machine regardless of the host os. 

This manual contains lots of screenshots and explains everything in detail in a way that by following the guide you will understand each step profoundly. 

It's great that even utilizing the firewall has been included in this tutorial along with strong emphasis on good password hygiene!

In my current line of work in public cloud technologies installing a Linux on VM like this is a little bit old fashioned. When building cloud native applications the direction is to rely on more modern virtualization and containerization technologies.
 
# a) Make-belief boogie-man. Create a treath model for imaginary company.

## What are we working on? 

Imaginary company Calibri Travels (known as “CT” from now on) is specialized in customized travel experienced in warm destinations of Southern Europe. CT’s business departments analysts demand a fast execution of client data and ML algorithms to predict travel spikes and scale up the services offerings according to the demand. CT’s main competitive advantage comes from its infrastructure where a client facing application running in CT’s own server halls (managed mostly by external company) is connected to Public Cloud Service Providers site by dedicated physical cable connection. The applications logic demands running it’s workloads partially on the CSP’s platforms for the analysis and the client data is replicated from the On Premises servers to CSP’s storage. 

The CT’s IT department tries it’s best to support the business departments needs without forgetting the best security practices. The workloads in the Public Cloud Service are not faced to internet and are only connected to the On Prem Server. The logic on Public Cloud Service between the database and ML / AI components is implemented internally and secured according to the Public Cloud Service Providers best practices.

CT’s main assets are the client data that is stored on Public Cloud Service Providers storage and the algorithm and logic how the data is used by AI/ML models to help the business department to do rapid and more enlightened decisions faster than the competitors. Business department also demands that by utilizing cloud native tools in data processing CT will achieve scalability and savings in costs and time. Without these CT would practically cease to exist and wouldn’t have anything that would separate them from other travel companies. 

<img width="902" alt="image" src="https://github.com/p1hkal/home/assets/12716380/7e940028-5c9e-4bba-b3d6-0e874b276625">

## What could go wrong in this model? The bad guys are mostly interested in the customer data that CT has acquired and built its business on. A possibility of state backed APTs can’t be ruled away, so the attack surface should be as minimal as possible and every control should be implemented to make lateral movement in the system impossible. By the help of STRIFE model here are some of the threats defined with mitigation plan:

## Spoofing is pretending to be something or someone you're not.

Authentication is always needed when making changes to customer details or data.

The authentication relies on MFA (Multi-Factor Authentication) to prevent adversaries to log in with stolen credentials.

Every connection uses SSL by principle.

By principle risks included to spoofing should be either mitigated or eliminated. Some of the risks is transferred in the architectural design of the whole environment and to networking and IAM configurations.

Even if the adversary could login to the Data Center, the connection between Data Center and Cloud Service is established with data cable which is managed by the Cloud Service. Therefore, some of the risk related to that connection is transferred to the Cloud Service Provider according to Shared Responsibility Model.

The application logic between Data Center and Cloud Service is abstracted behind API and the API calls are done by Service Accounts and not by humans. The Service Account keys are rotated periodically, and all the data is encrypted in transaction and rest. These controls are in place for a situation where the attacker gains foothold in the Data Center.

The management of encryption keys is shared between CT and Cloud Service Provider depending on the service and the use case. 

## Tampering is modifying something you're not supposed to modify. It can include packets on the wire (or wireless), bits on disk, or the bits in memory.

Controls related to tampering are in place in Data Center and Cloud Service Provider, both have ACL’s in place configured according to the principle of least privilege. 

Some OS level risks are transferred to the Cloud Service Provider, but ACLs and digital signatures are used along with hardened OS images and encrypted computing when possible.

## Repudiation means claiming you didn't do something (regardless of whether you did or not).

In a hybrid cloud environment where workloads and application logic is scattered between different locations and data centers it can’t be stressed how important of a well-managed log trail is.

CT has to implement a SIEM solution for centralized logging and make sure that the logging covers all the environments.

The logging has to have a good coverage, but must also be implemented in a way that the logs won’t be compromised. Every service and environment should have a documentation that explains what logs are gathered and where and how they are sent.

## Denial of Service are attacks designed to prevent a system from providing service, including by crashing it, making it unusably slow, or filling all its storage.

The entry point from internet to the application has to be secured with firewall and load balancer.

This point of connection has to be monitored at all times and there has to be alerts in a situation where a DoS attack occurs.

The load balancer will also establish a SSL connection between itself and the Data Center to create a security perimeter for authenticated users.

DoS attacks should be mitigated at this point. Every decision related to the  architecture, design, choosing the vendor or right hardware at this point of the data flow should be a subject to this goal.

If there would be a DoS attack that would disturb the data cable connection between Data Center and Cloud Service, in this situation some of the risk is transferred to the Cloud Service according to the shared responsibility model, however all the services running in the Cloud Platform are highly scalable in case of DoS and additional firewall can be implemented on this connection. 

## Information Disclosure is about exposing information to people who are not authorized to see it.

As a company which handles a vast amount of business critical and personal customer data, there's a big risk in exposing private data. The network and the interfaces and connections between the Data Center and Cloud Service should be seen as weak links along with the point of entry to the Data Center. Data should always be encrypted in transit and rest, the certificates and encryption keys should be managed and accessed appropriately.

The network traffic should be monitored between different sites and data should be classified and obfuscated if needed. CT should also have well defined ACL's, roles and IAM processes to make sure that every access and privilege is granted as granularly as possible.

The API's should be secured and hardened in a way that all the application logic would be abstracted away and no unnecessary information would be leaked by accident.

## Elevation of Privilege is when a program or user is technically able to do things that they're not supposed to do.













# b) Incident analyses. 
# c) Starting a lab.

# d) Voluntary bonus.

## Due to the holistic nature of security, there are several ways to approach the topic and many ways to consider which theoretical foundation to prioritize. For a person who considers security from legal perspective GDPR, NIS2 or other standards or legal requirements will matter more than to a technical person who would be more intersted in how to implement Zero Trust networks, IAM policies according to the principle of least privilege or how to secure the organizations web apps from OWASP10 threats. 

If I would have to choose what concepts to teach about modern information security on a first day I would start with the idea of Zero Trust. This is the way of implementing security in cloud environments by assuming that every workload and every connection is potentially under a threat and actually assuming that the breach has already happened. Every transaction has to be authenticated and monitored and every access right, was it firewall opening or group membership of an identity, should be implemented with narrowest possible rights that is possible in the usecase. I think this is a good way of viewing security in modern environments and a good starting point.

Another topic which I would present is the shared responsibility model as a concept. This model defines the responsibilites in IaaS, SaaS & PaaS environments. I think this model is crucial when trying to understand what technical components fall into the customers responsibilities and what are completely managed by a cloud service provider such as AWS or GCP. When considering the shared responsibility of managed kubernetes engines like GKE and EKS these roles can be confusing for many organizations when assessing the security and risks on these technologies. 



